{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krzysztof-kar/ML2025/blob/main/Lab07_gradient-boosting_prdom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework Assignment: Understanding Binary Cross-Entropy in the Forward Stagewise Procedure\n",
        "-----------------------------\n",
        "\n",
        "## The Problem\n",
        "\n",
        "In this assignment, you will demonstrate that adding a new constant predictor in the forward stagewise procedure to an already existing predictor from a previous stage, $f_{m-1}(x_i)$ (which we will denote as $f_i$ for ease of notation), may be for certain loss functions fundamentally more challenging than building a constant predictor from scratch. You will work with the binary cross-entropy loss defined as\n",
        "\n",
        "$$\n",
        "L(y, z) = -y \\log(\\sigma(z)) - (1 - y) \\log(1 - \\sigma(z)),\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $y_i \\in \\{0,1\\}$ are the binary labels,\n",
        "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\n",
        "\n",
        "For the purposes of this assignment, assume that you are given:\n",
        "\n",
        "- A dataset of binary labels $y_i$.\n",
        "- **Two constants: $m$ (the number of ones) and $k$ (the number of zeros)** in the labels in the dataset.\n",
        "- A set of predictions $f_i = f_{m-1}(x_i)$ obtained from a previous stage, where the $f_i$ values are generated randomly from a normal distribution.\n",
        "\n",
        "### The assignment will explore two scenarios:\n",
        "\n",
        "### Scenario A: Fitting a Constant Predictor from Scratch\n",
        "\n",
        "In this scenario, you are building a predictor from scratch. The task is to find the optimal constant value $\\lambda$ that minimizes the binary cross-entropy loss over the dataset. Formulate the optimization problem as:\n",
        "\n",
        "$$\n",
        "\\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, \\lambda).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Scenario B: Fitting the m-th Predictor in the Forward Stagewise Procedure\n",
        "\n",
        "Now assume you already have an existing predictor $f_i = f_{m-1}(x_i)$. Rather than predicting from scratch, you wish to find an optimal additive correction $\\lambda$ such that the updated prediction for each data point becomes\n",
        "\n",
        "$$\n",
        " f_i + \\lambda,\n",
        "$$\n",
        "\n",
        "and the corresponding binary cross-entropy loss is given by\n",
        "\n",
        "$$\n",
        "\\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, f_i + \\lambda).\n",
        "$$\n",
        "\n",
        "\n",
        "## Research Questions\n",
        "\n",
        "In this assignment you will answer the following questions:\n",
        "\n",
        "- Why is finding an optimal additive shift $\\lambda$ in the forward stagewise procedure fundamentally harder than directly fitting a single-parameter predictor from scratch?\n",
        "- How does the complexity of the loss landscape differ between these two scenarios? Discuss the differences in the shape and smoothness of the loss function in both cases.\n",
        "\n",
        "## Tasks & Deliverables\n",
        "\n",
        "1. **Derivation and Analysis**\n",
        "\n",
        "  **Scenario A:**\n",
        "  - Derive explicitly the optimal $\\lambda$ for fitting from scratch, i.e., solve\n",
        "    \n",
        "    $$\n",
        "    \\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, \\lambda)\n",
        "    $$\n",
        "    \n",
        "    and express the answer in terms of $m$ and $k$.\n",
        "    *Hint:* First, write the derivative of the loss with respect to $\\lambda$ and set it to zero to obtain an implicit equation.\n",
        "\n",
        "  - **Interpretation:**\n",
        "  Provide a clear interpretation of your derived optimal $\\lambda$. What does this constant represent in terms of the dataset's label distribution?\n",
        "\n",
        "  **Scenario B:**\n",
        "  - Derive the implicit equation that $\\lambda$ must satisfy in the additive shift scenario:\n",
        "    \n",
        "    $$\n",
        "    \\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, f_i + \\lambda).\n",
        "    $$\n",
        "    *Hint:* First, write the derivative of the loss with respect to $\\lambda$ and set it to zero to obtain an implicit equation.\n",
        "\n",
        "  - **Discussion:**\n",
        "    Explain clearly why this implicit equation has no simple closed-form solution, unlike the previous case. What role does the non-linearity of the sigmoid function (in the binary cross-entropy loss) play in this difficulty?\n",
        "\n",
        "2. **Loss Landscape Exploration (Python)**\n",
        "\n",
        "  Write a Python code that:\n",
        "\n",
        "  - Uses provided values for $n$, and for $k$ and $m$ (the number of zeros and ones in the labels, respectively), $k+m=n$.\n",
        "  - Generates a set of predictions $f_i$ by sampling from a normal distribution.\n",
        "  - Plots the binary cross-entropy loss as a function of $\\lambda$ for:\n",
        "    - Scenario A:\n",
        "      $$\n",
        "      \\text{plot } \\sum_{i=1}^{n} L(y_i, \\lambda) \\text{ as a function of }\\lambda\n",
        "      $$\n",
        "    - Scenario B:\n",
        "      $$\n",
        "      \\text{plot } \\sum_{i=1}^{n} L(y_i, f_i + \\lambda) \\text{ as a function of }\\lambda\n",
        "      $$\n",
        "  - **Discussion:**\n",
        "  Is the loss landscape in Scenario A simpler or more complex than in Scenario B? Is it multimodal or unimodal? If so, is it thinkable the lambda minimizer in Scenario B can be found numerically? Where does the difficulty in Scenario B come from: the non-linearity of the problem or a complex loss landscape?\n",
        "\n",
        "3. **Report**  \n",
        "   - Summarize your theoretical insights and empirical findings in a **Colab notebook**.\n",
        "   - Include the relevant proofs, code, discussion, and conclusions.\n",
        "   - Place the notebook in your **GitHub repository** for this course, add a link to it in your README.md and add an **“Open in Colab”** badge in the notebook so it can be launched directly.\n",
        "\n"
      ],
      "metadata": {
        "id": "L9VT1gISt5ZU"
      },
      "id": "L9VT1gISt5ZU"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}